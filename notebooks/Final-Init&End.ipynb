{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project for Adaptive Decision Making\n",
    "### Modeling Behavioral Data from Attention Decay Study with Single-boundary DDM\n",
    "Group Members: Oscar, Emilio, and William"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Problem \n",
    "\n",
    "+ **Research Question 1 ** What can DDM Modeling tell us about the cognitive process of perceptual decision making?\n",
    "+ **Research Question 2 ** Can we fit a single-boundary DDM to our empirical Reaction Time (RT) distribution?\n",
    "+ **Research Question 3 ** Can we Modify the DDM to account for demonstrated attention decay in our empirical data? Will this produce a better fit? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling\n",
    "\n",
    "#### Single decision boundary DDM\n",
    "This was created by basically ignoring the lower or \"no\" decision boundary. The modifications to the general DDM algorithm are such:\n",
    "- If the evidence ever falls below zStart, set the evidence to zStart\n",
    "- Remove the loop breaking logic for a \"no\" decision\n",
    "\n",
    "#### Decay parameter and decay acceleration\n",
    "We wanted to build in the parameters for attention decay. The intuition is that people will lose focus and pay less attention. The amount of attention that they give to the task will lessen over time such that the speed of evidence gathering is decreasing as the number of trials increases. \n",
    "We decided to go with a simple decay model:\n",
    "- Each trial has an associated decay such that at each step of the evidence gathering, the decision to save a drowning face will get dragged towards \"no decision made\" by the decay\n",
    "- After each trial, the amount of decay increases by the decay acceleration\n",
    "\n",
    "#### Simulating the DDM\n",
    "Once these models were made, we created algorithms for calculating the chi-square formulas for the dataframe that we had from the behavioral study and the dataframe that we generated with these DDM simulations.\n",
    "\n",
    "From there we can simulate the DDM and compare it against the behavioral dataframes and tune the parameters to minimize the chi-square and maximize the p-value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Do all of our main imports first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from ADMCode import visualize as vis\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numba as nb\n",
    "\n",
    "from numba.decorators import jit\n",
    "from numpy.random import random_sample\n",
    "from numba import float64, int64, vectorize, boolean\n",
    "\n",
    "from ipywidgets import interactive\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter('ignore', np.RankWarning)\n",
    "warnings.filterwarnings(\"ignore\", module=\"matplotlib\")\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style='white', font_scale=1.3)\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "Trial = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single decision barrier DDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nb.typeof((1.0, 1.0))(float64[:], float64[:], float64[:]), nopython=True)\n",
    "def sim_ddm(parameters, rProb, trace):\n",
    "\n",
    "    # extract parameters\n",
    "    a, tr, v, z, si, dx, dt = parameters\n",
    "\n",
    "    # convert drift-rate into a probability,\n",
    "    # & scale by sigma (si) and timestep (dt)\n",
    "    # if v > 0, then 0.5 < vProb < 1.0\n",
    "    # if v < 0, then 0.0 < vProb < 0.5\n",
    "    vProb = .5 * (1 + (v * np.sqrt(dt))/si)\n",
    "\n",
    "    # define starting point with respect to boundary height\n",
    "    zStart = z * a\n",
    "\n",
    "    #initialize evidence variable at zStart\n",
    "    evidence = zStart\n",
    "    trace[0] = evidence\n",
    "\n",
    "    # define deadline (max time allowed for accumulation)\n",
    "    deadline = trace.size\n",
    "\n",
    "    for nsteps in range(1, deadline):\n",
    "        # sample a random probability (r) and compare w/ vProb\n",
    "        if rProb[nsteps] < vProb:\n",
    "            # if r < vProb, step evidence up (towards a)\n",
    "            evidence += dx\n",
    "        else:\n",
    "            # if r > vProb, step evidence down (towards 0)\n",
    "            evidence -= dx\n",
    "\n",
    "        if evidence < zStart:\n",
    "            evidence = zStart\n",
    "            \n",
    "        # store new value of evidence at current timestep\n",
    "        trace[nsteps] = evidence\n",
    "            \n",
    "        if evidence >= a:\n",
    "            # calculate RT (in milliseconds)\n",
    "            rt = tr + (nsteps * dt)\n",
    "            # set choice to 1.0 (upper bound)\n",
    "            choice = 1.0\n",
    "\n",
    "            # terminate simulation, return rt & choice\n",
    "            return rt, choice\n",
    "\n",
    "    # return -1.0 for rt and choice so we can filter out\n",
    "    # trials where evidence never crossed 0 or a\n",
    "    return -1.0, -1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background code for running DDM and generating dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_ddm_storage_objects(parameters, ntrials=200, deadline=1.5):\n",
    "    dt = parameters[-1]\n",
    "    ntime = np.int(np.floor(deadline / dt))\n",
    "    data = np.zeros((ntrials, 2))\n",
    "    rProb = random_sample((ntrials, ntime))\n",
    "    traces = np.zeros_like(rProb)\n",
    "    return data, rProb, traces\n",
    "\n",
    "def clean_output(data, traces, deadline=1.2, stimulus=None):\n",
    "    df = pd.DataFrame(data, columns=['rt', 'choice'])\n",
    "    df.insert(0, 'trial', np.arange(1, 1+df.shape[0]))\n",
    "    df = df[(df.rt>0)&(df.rt<deadline)]\n",
    "    traces = traces[df.index.values, :]\n",
    "    df = df.reset_index(drop=True)\n",
    "    return df, traces\n",
    "\n",
    "@jit((float64[:], float64[:,:], float64[:,:], float64[:,:]), nopython=True)\n",
    "def _sim_ddm_trials_(parameters, data, rProb, traces):\n",
    "    ntrials = data.shape[0]\n",
    "    for t in range(ntrials):\n",
    "        data[t, :] = sim_ddm(parameters, rProb[t], traces[t])\n",
    "\n",
    "def sim_ddm_trials(parameters, ntrials=500, deadline=1.5, decay=False):\n",
    "    data, rProb, traces = gen_ddm_storage_objects(parameters, ntrials, deadline)\n",
    "    _sim_ddm_trials_(parameters, data, rProb, traces)\n",
    "    df, traces = clean_output(data, traces, deadline=deadline)\n",
    "    return df, traces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize our parameters\n",
    "We can begin with a regular DDM (with one boundary) and tweak with the parameters or potentially add more parameters to get this best fit to the behavioral data that we have"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = .09 # boundary height\n",
    "v = .25 # strong drift-rate\n",
    "tr = .15 # nondecision time (in seconds)\n",
    "z = 0 # starting point ([0,1], fraction of a)\n",
    "\n",
    "dt = .001 # time step\n",
    "si = .1 # sigma (noise scalar)\n",
    "dx = si * np.sqrt(dt) # evidence step\n",
    "deadline = 2 # max decision time\n",
    "ntime = np.int(np.floor(deadline / dt)) # time limit for decision\n",
    "ntrials = 100 # number of trials to simulate\n",
    "\n",
    "\n",
    "\n",
    "parameters = np.array([a, tr, v, z, si, dx, dt])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df, traces = sim_ddm_trials(parameters, ntrials, deadline)\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = df.choice.mean()\n",
    "corRT = df[df.choice==1].rt.mean()\n",
    "y,x = df.shape\n",
    "count = 0 \n",
    "print(y)\n",
    "\n",
    "\n",
    "while count <y:\n",
    "\n",
    "    df[\"rt\"][count] =  (df[\"rt\"][count])\n",
    "    \n",
    "    count+=1\n",
    "\n",
    "\n",
    "print(\"RT (cor) = {:.0f} ms\".format(corRT/dt))\n",
    "print(\"Accuracy = {:.0f}%\".format(accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization of single barrier DDM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = vis.plot_this_sims(df, parameters,traces = traces, plot_v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a single decision barrier DDM with attention decay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit(nb.typeof((1.0, 1.0))(float64[:], float64[:], float64[:]), nopython=True)\n",
    "def sim_decay_ddm(parameters, rProb, trace):\n",
    "    # extract parameters\n",
    "    a, tr, v, z, si, dx, dt, decay, decay_acceleration = parameters\n",
    "\n",
    "    # convert drift-rate into a probability,\n",
    "    # & scale by sigma (si) and timestep (dt)\n",
    "    # if v > 0, then 0.5 < vProb < 1.0\n",
    "    # if v < 0, then 0.0 < vProb < 0.5\n",
    "    vProb = .5 * (1 + (v * np.sqrt(dt))/si)\n",
    "\n",
    "    # define starting point with respect to boundary height\n",
    "    zStart = z * a\n",
    "\n",
    "    #initialize evidence variable at zStart\n",
    "    evidence = zStart\n",
    "    trace[0] = evidence\n",
    "\n",
    "    # define deadline (max time allowed for accumulation)\n",
    "    deadline = trace.size\n",
    "\n",
    "    for nsteps in range(1, deadline):\n",
    "        # sample a random probability (r) and compare w/ vProb\n",
    "        if rProb[nsteps] < vProb:\n",
    "            # if r < vProb, step evidence up (towards a)\n",
    "            evidence += dx\n",
    "        else:\n",
    "            # if r > vProb, step evidence down (towards 0)\n",
    "            evidence -= dx\n",
    "\n",
    "        # always add in decay parameter\n",
    "        evidence -= (decay + decay_acceleration * Trial)\n",
    "            \n",
    "        if evidence < zStart:\n",
    "            evidence = zStart\n",
    "            \n",
    "        # store new value of evidence at current timestep\n",
    "        trace[nsteps] = evidence\n",
    "            \n",
    "        if evidence >= a:\n",
    "            # calculate RT (in milliseconds)\n",
    "            rt = tr + (nsteps * dt)\n",
    "            # set choice to 1.0 (upper bound)\n",
    "            choice = 1.0\n",
    "\n",
    "            # terminate simulation, return rt & choice\n",
    "            return rt, choice\n",
    "\n",
    "    # return -1.0 for rt and choice so we can filter out\n",
    "    # trials where evidence never crossed 0 or a\n",
    "    return -1.0, -1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Background code for running decay DDM and generating dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@jit((float64[:], float64[:,:], float64[:,:], float64[:,:]), nopython=True)\n",
    "def _sim_decay_ddm_trials_(parameters, data, rProb, traces):\n",
    "    ntrials = data.shape[0]\n",
    "    for t in range(ntrials):\n",
    "        Trial = t\n",
    "        data[t, :] = sim_decay_ddm(parameters, rProb[t], traces[t])\n",
    "\n",
    "def sim_decay_ddm_trials(parameters, ntrials=500, deadline=1.5, decay=False):\n",
    "    data, rProb, traces = gen_ddm_storage_objects(parameters, ntrials, deadline)\n",
    "    _sim_decay_ddm_trials_(parameters, data, rProb, traces)\n",
    "    df, traces = clean_output(data, traces, deadline=deadline)\n",
    "    return df, traces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = .3 # boundary height\n",
    "v = .40 # strong drift-rate\n",
    "tr = 0 # nondecision time (in seconds)\n",
    "z = 0 # starting point ([0,1], fraction of a)\n",
    "\n",
    "dt = .001 # time step\n",
    "si = .1 # sigma (noise scalar)\n",
    "dx = si * np.sqrt(dt) # evidence step\n",
    "deadline = 5 # max decision time\n",
    "ntime = np.int(np.floor(deadline / dt)) # time limit for decision\n",
    "ntrials = 1000 # number of trials to simulate\n",
    "decay = 0.000005\n",
    "decay_acceleration = 0.00001\n",
    "\n",
    "\n",
    "parameters_decay = np.array([a, tr, v, z, si, dx, dt, decay, decay_acceleration])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decayDf, traces = sim_decay_ddm_trials(parameters_decay, ntrials, deadline)\n",
    "meanRT = decayDf.rt.mean()\n",
    "\n",
    "stdDevRT = decayDf.rt.std()\n",
    "\n",
    "print(\"RT (average) = {:.0f} ms\".format(meanRT/dt))\n",
    "print('stdDev = {:.0f} ms'.format(stdDevRT/dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bx = vis.plot_this_sims(decayDf, parameters,traces = traces, plot_v=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Our Approach to Model Accuracy Estimation \n",
    "Now that we've created a single boundary DDM capable of producing simulated Reaction Time distribution, we want to begin manipulating its parameters to fit it to our experimental data. In order to do that in a principled way, however, we need a way of measuring if a particular set of parameters produces a better fit on the experimental data than another set of parameters. For this we will use a Chi Square 'goodness of fit' test. \n",
    "\n",
    "#### TODO: $$Formula For Chi Square Goes Here$$ \n",
    "\n",
    "The goal for our model is to minimize the value of the Chi Square, as it represents the likelihood that the distribution produced by the model is the same distribution from the experimental data.\n",
    "\n",
    "### Chi Square to Calculate Goodnes of Fit\n",
    "At a conceptual level, we are comparing how closely the observed (experimental) data matches the expected (simulation) data. We calculate the probability of any differences between the datasets being present, if they are indeed sampled from the same distribution. \n",
    "\n",
    "#### TODO: Visualization of Observed vs Expected distribution, can use lecture slides\n",
    "\n",
    "\n",
    "\n",
    "+ **Note** here, that because we are using a single output of the model simulation, a model that inherently has a high variance between Simualtion instances can have significantly different $\\chi^2 $ value depending on which instance we use calculate this statistic. As such, averaging the distributions between simulations of such a model increases reliability. \n",
    "\n",
    "### 'Bucketing' the data\n",
    "For us to be able to calculate the $\\chi^2$ fit we need to choose RT intervals within which to 'bucket' data points. Intuitively, the more fine-grained the buckets, the snugger the fit. But there is a catch. For this calculation to have statistical validity we must have at least 5 data points within each bucket. While this is not a problem for simulation data, it is a limitation for our experimental data since we only have so many trials. \n",
    "\n",
    "This is a key constraint that has downstream effects on what we choose to model and what comparisons we draw. Particularly, individual subject modeling now becomes a challnge, as we only have 47 data points for the entire experiment (3 for the pre-test period, 41 for the experimental period, and 3 for the post-test period). This is mostly due to flaws in the experimental design, and of course, the researchers have no clue that the data was going to be modeled in this way. \n",
    "\n",
    "#### TODO: Present our RT intervals and the rationale/process for arriving at them \n",
    "\n",
    "\n",
    "\n",
    "### The algorithm for calculating the goodness of fit\n",
    "Special thanks to Wikipedia, Khan Academy and Tim and Kyle for the surprinsingly informative vizualization of the $\\chi^2$ estimation on the DDM lecture slides. \n",
    "\n",
    "1. Define the buckets for your RT intervals. \n",
    "2. Collect your data points into the buckets. Do this for the experimental data and the simulated data (using the same intervals for both data sets).  \n",
    "3. Count the Data points inside the intervals to make sure there is more than 5 in each bucket (Large Counts condition)\n",
    "4. For each interval calculate $(observed counts - expected counts)/expected counts$, and then sum them all to calculate the Chi Square of your model. \n",
    "$$ \\chi^s = SumFormulaHere $$\n",
    "5. Get the degrees of freedom of this Chi Square distribution by subtracting one from your number of buckets. \n",
    "6. Compare your Chi Square against the critical Chi Square values for different values of $\\alpha$. They represent the likelihood that your simulated and observed data come from the same distribution. \n",
    "    * There is a $\\alpha$ percent chance that if the two datasets come from the same distribution, their differences would be what they are or more extreme\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up goodness of fit calculation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing and Formatting experimental data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "dataPath = \"data/RTData.csv\"\n",
    "data = pd.read_csv(dataPath, header = 0)\n",
    "data = data.dropna()\n",
    "\n",
    "dataWhole = data[data.isFreq == 1]\n",
    "dataisV = dataWhole[dataWhole.isVar==1]\n",
    "dataisNV = dataWhole[dataWhole.isVar==0]\n",
    "dataisInit = data[data.timePeriod == 'Initial']\n",
    "dataisEnd = data[data.timePeriod == 'End']\n",
    "\n",
    "def changeD(rData): \n",
    "    rRT = \"clickDelay\"\n",
    "    rt = \"rt\"\n",
    "    trial = []\n",
    "    newRT = []\n",
    "    newCh = []\n",
    "    dShape,garb = rData.shape\n",
    "    trial = [i for i in range(dShape)]\n",
    "    newRT = list(rData[rRT])\n",
    "\n",
    "    newCh = [1 for i in range(dShape)]\n",
    "    newDf = {\"trial\":trial,\"rt\":newRT,\"choice\":newCh}\n",
    "    return pd.DataFrame(data = newDf)\n",
    "dataAll,dataVar,dataNVar,dataInit,dataEnd = changeD(dataWhole),changeD(dataisV),changeD(dataisNV),changeD(dataisInit),changeD(dataisEnd)\n",
    "\n",
    "#add identifiers \n",
    "dataAll['id'] = 'All'\n",
    "dataVar['id'] = 'Var'\n",
    "dataNVar['id'] = 'NVar'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Buckets\n",
    "Our first thought to define our 'buckets' was by setting them to be the width of one standard deviation, with two 'catch all' buckets for the distribution tails. After some testing, these proved to be a bit too wide so we set them to half of a standard deviation. \n",
    "\n",
    "We experimented by using the standard error $\\sigma/\\sqrt{N}$, but it proved too small. \n",
    "\n",
    "After gathering the RT counts within the buckets we realized we had to collapse the initial buckets into one because the contained too few counts. As such they would violate the large counts assumption of the Chi Square model and ruin our **statistical validity**. This is due to the experimental data having a rightward skew. \n",
    "\n",
    "#### Our initial buckets\n",
    "Experimental buckets:\n",
    "+ All data buckets =  [0, 0, 0, 16, 108, 70, 69, 44, 27, 20, 15, 16, 31]\n",
    "+ Variable condition buckets =  [0, 0, 0, 22, 54, 33, 31, 30, 14, 9, 12, 8, 20]\n",
    "+ Sable condition buckets =  [0, 0, 0, 2, 47, 43, 30, 18, 10, 8, 3, 6, 16]\n",
    "\n",
    "We need at least 5 data points in each bucket. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def stdDev(mean,L):\n",
    "    Sum = 0\n",
    "    for i in L: \n",
    "        Sum+=math.pow((i-mean),2)\n",
    "    return math.sqrt((Sum/len(L)))\n",
    "\n",
    "\n",
    "def bucketer(mean,stdD,n):\n",
    "    conMax = 3 #confidence rating \n",
    "    n = math.sqrt(n)\n",
    "    bucket = []\n",
    "    low = -1*conMax\n",
    "    high = conMax+1 \n",
    "    for i in [j for j in range(low,high)]:\n",
    "        term = mean + i*(stdD/n)\n",
    "        bucket.append(term)\n",
    "    return bucket\n",
    "\n",
    "def sampleBasedBucketer(mean, stdD):\n",
    "    distance = 6 # 2 std devs, since it will be halved below\n",
    "    bounds = []\n",
    "    for i in range(-distance, distance):\n",
    "        term = mean + i * (stdD/3)\n",
    "        bounds.append(term)    \n",
    "    return bounds\n",
    "\n",
    "\n",
    "def getSampleBounds(panDf):\n",
    "    mean = panDf.rt.mean()\n",
    "    stdDev = panDf.rt.std()\n",
    "    print(\"sample Mean = {:.3f} s\".format(mean))\n",
    "    print(\"sample Std Dev = {:.3f} s\".format(stdDev))\n",
    "    return sampleBasedBucketer(mean, stdDev)\n",
    "           \n",
    "def getStats(panDf): \n",
    "    y,x = panDf.shape\n",
    "    Sum = 0\n",
    "    count =0\n",
    "    new = []\n",
    "    while count<y:\n",
    "        ind = panDf[\"rt\"][count]\n",
    "        Sum+= ind\n",
    "        new+=[ind]\n",
    "        count+=1\n",
    "    mean = Sum/count\n",
    "    std = stdDev(mean,new)\n",
    "    n = len(new)\n",
    "    buck = bucketer(mean,std,n)\n",
    "    return mean,std,buck\n",
    "\n",
    "testM,testD,testB = getStats(decayDf)\n",
    "\n",
    "sampleBounds = getSampleBounds(decayDf)\n",
    "\n",
    "#print('std Error buckets')\n",
    "#for bound in testB:\n",
    "#    print(bound)\n",
    "\n",
    "#print('sample std Dev buckets')\n",
    "#for bound in sampleBounds:\n",
    "#    print(bound)\n",
    "    \n",
    "print(\"simulated Mean = {:.3f} s\".format(testM))\n",
    "print(\"simulated Std Dev = {:.3f} s\".format(testD))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Count the buckets into an histogram-like array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def isInBounds(lower, upper, value):\n",
    "    return lower <= value and value <= upper\n",
    "\n",
    "def getBuckets(bounds, dataframe):\n",
    "    buckets = [0] * (len(bounds) + 1) \n",
    "    bounds.sort()\n",
    "    boundsList = [] # [(lower1, upper1), (lower2, upper2) ...]\n",
    "    for i,bound in enumerate(bounds):\n",
    "        if i == 0:\n",
    "            boundsList.append((0, bound))\n",
    "        else:\n",
    "            boundsList.append((bounds[i-1], bound))\n",
    "        #add right tail end bucket\n",
    "        if i == len(bounds) - 1:\n",
    "            boundsList.append((bound, 100)) #100 is a catchAll \n",
    "            \n",
    "    #print(dataframe.shape)\n",
    "    for i,(lower, upper) in enumerate(boundsList):\n",
    "        for rt in dataframe.rt.values:\n",
    "            if isInBounds(lower, upper, rt):\n",
    "                buckets[i] += 1\n",
    "    return buckets\n",
    "\n",
    "obsvMean,obsvDev,obsvBuck = getStats(dataAll)\n",
    "\n",
    "sampleObsvBuck = getSampleBounds(dataAll)\n",
    "\n",
    "simBucketCount = getBuckets(testB, decayDf)\n",
    "obsvBucketCount = getBuckets(obsvBuck, dataAll)\n",
    "obsvSampleBucketCount = getBuckets(sampleObsvBuck, dataAll)\n",
    "simSampleBucketCount = getBuckets(sampleBounds, decayDf)\n",
    "\n",
    "\n",
    "\n",
    "#print(\"simulated buckets: \\n\", simBucketCount)\n",
    "print(\"simulated buckets sample based: \\n\", simSampleBucketCount)\n",
    "\n",
    "\n",
    "print(\"\\n All data experimental buckets: \\n\", obsvBucketCount)\n",
    "print(\"All experimental  buckets sample based: \\n\", obsvSampleBucketCount)\n",
    "\n",
    "#using sample based from now on\n",
    "\n",
    "print(\"\\nExperimental buckets:\")\n",
    "allDataBuckets = getBuckets(getSampleBounds(dataAll), dataAll)\n",
    "varDataBuckets = getBuckets(getSampleBounds(dataVar), dataVar)\n",
    "staDataBuckets = getBuckets(getSampleBounds(dataNVar), dataNVar)\n",
    "print(\"All data buckets = \", allDataBuckets)\n",
    "print(\"Variable condition buckets = \", getBuckets(getSampleBounds(dataVar), dataVar))\n",
    "print(\"Sable condition buckets = \", getBuckets(getSampleBounds(dataNVar), dataNVar))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculate Chi Square\n",
    "while making sure to not violete the Large Counts rule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## calculate Chi squares\n",
    "import scipy\n",
    "from scipy import stats\n",
    "\n",
    "def collapseUpToFive(L):\n",
    "    firstCount = L[0]\n",
    "    i = 1\n",
    "    while firstCount <= 5:\n",
    "        firstCount += L[i]\n",
    "        i += 1\n",
    "    return i - 1\n",
    "\n",
    "#print(collapseUpToFive([0, 0, 0, 22, 54, 33, 31, 30, 14, 9, 12, 8, 20]))\n",
    "#print(collapseUpToFive([0, 0, 0, 2, 47, 43, 30, 18, 10, 8, 3, 6, 16]))\n",
    "\n",
    "\n",
    "def getChiSquare(observedDf, expectedDf):\n",
    "    #get ObservedBounds\n",
    "    bounds = getSampleBounds(observedDf)\n",
    "    # get buckets for both using observedBounds\n",
    "    observedBuckets = getBuckets(bounds, observedDf)\n",
    "    truncator = collapseUpToFive(observedBuckets)\n",
    "    adjustedBounds = bounds[truncator:]\n",
    "    observedBuckets = getBuckets(adjustedBounds, observedDf)\n",
    "    expectedBuckets = getBuckets(adjustedBounds, expectedDf)\n",
    "    \n",
    "    assert(len(observedBuckets) == len(expectedBuckets))\n",
    "    \n",
    "    \n",
    "    \n",
    "    observed_values = scipy.array(observedBuckets)\n",
    "    expected_values = scipy.array(expectedBuckets)\n",
    "    chiSquare, pValue = scipy.stats.chisquare(observed_values, f_exp=expected_values)\n",
    "    print(\"observed buckets =\", observedBuckets)\n",
    "    print(\"expected buckets = \", expectedBuckets)\n",
    "    print(\"chi Square = {:3f}\".format(chiSquare))\n",
    "    print(\"pvalue = {:3f}\".format(pValue))\n",
    "    return chiSquare, pValue \n",
    "\n",
    "#test chi square:\n",
    "getChiSquare(dataVar, dataVar) \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vizualize the data to build intuition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plotting the raw Data \n",
    "allDataAx = sns.distplot(dataAll.rt.values).set_title('All Data')\n",
    "\n",
    "#sample Mean = 3.026 s\n",
    "#sample Std Dev = 2.304 s\n",
    "#adj bounds =  [0.7220, 1.4900, 2.2581, 3.02624, 3.7943, 4.5623, 5.330, 6.09854, 6.8666]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Variable Statistics\n",
    "#sample Mean = 3.200 s\n",
    "#sample Std Dev = 2.540 s\n",
    "#adj bounds (sec) = , 1.507, 2.354, 3.200, 4.047, 4.893, 5.740, 6.586, 7.433\n",
    "\n",
    "staDataAx = sns.distplot(dataNVar.rt.values, color='r').set_title('Non Var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Variable Statistics\n",
    "#sample Mean = 2.890 s\n",
    "#sample Std Dev = 2.096 s\n",
    "#adj bounds (sec) =  0.793, 1.492, 2.191, 2.890, 3.588, 4.287, 4.986, 5.685, 6.384\n",
    "\n",
    "varDataAx = sns.distplot(dataVar.rt.values, color='g').set_title('Var')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plotting the raw Data for var and sta\n",
    "varDataAx = sns.distplot(dataVar.rt.values, color='g', label='Var')\n",
    "staDataAx = sns.distplot(dataNVar.rt.values, color='r', label='Non Var')\n",
    "legend, title = plt.legend(), plt.title('variable vs Stable')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameter Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Single-Bound DDM - Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "a = .5 # boundary height\n",
    "v = .0235 # strong drift-rate\n",
    "tr = .15 # nondecision time (in seconds)\n",
    "z = 0 # starting point ([0,1], fraction of a)\n",
    "\n",
    "dt = .001 # time step\n",
    "si = .3 # sigma (noise scalar)\n",
    "dx = si * np.sqrt(dt) # evidence step\n",
    "deadline = 10 # max decision time\n",
    "ntime = np.int(np.floor(deadline / dt)) # time limit for decision\n",
    "ntrials = 233 # number of trials to simulate\n",
    "\n",
    "\n",
    "\n",
    "parameters = np.array([a, tr, v, z, si, dx, dt])\n",
    "\n",
    "simVarDf, traces = sim_ddm_trials(parameters, ntrials, deadline)\n",
    "cs, p = getChiSquare(dataVar, simVarDf)\n",
    "\n",
    "Sum = 0\n",
    "\n",
    "for test in range(100):\n",
    "    meanRT = simVarDf.rt.mean()\n",
    "    stdDevRT = simVarDf.rt.std()\n",
    "\n",
    "    print(\"RT (average) = {:.0f} ms\".format(meanRT/dt))\n",
    "    print('stdDev = {:.0f} ms'.format(stdDevRT/dt))\n",
    "    simVarDf, traces = sim_ddm_trials(parameters, ntrials, deadline)\n",
    "    cs,p = getChiSquare(dataVar, simVarDf)\n",
    "    print()\n",
    "    Sum += p\n",
    "print(\"Mean p-value:\", (Sum/100))\n",
    "\n",
    "varx = vis.plot_this_sims(df, parameters,traces = traces, plot_v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decay Single-Bound DDM - Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = .5 # boundary height\n",
    "v = .0235 # strong drift-rate\n",
    "tr = .15 # nondecision time (in seconds)\n",
    "z = 0 # starting point ([0,1], fraction of a)\n",
    "\n",
    "dt = .001 # time step\n",
    "si = .3 # sigma (noise scalar)\n",
    "dx = si * np.sqrt(dt) # evidence step\n",
    "deadline = 10 # max decision time\n",
    "ntime = np.int(np.floor(deadline / dt)) # time limit for decision\n",
    "ntrials = 233 # number of trials to simulate\n",
    "decay = 0.000005\n",
    "decay_acceleration = 0.00001\n",
    "\n",
    "\n",
    "parameters_decay = np.array([a, tr, v, z, si, dx, dt, decay, decay_acceleration])\n",
    "\n",
    "simDecayVarDf, traces = sim_decay_ddm_trials(parameters_decay, ntrials, deadline)\n",
    "cs, p = getChiSquare(dataVar, simDecayVarDf)\n",
    "\n",
    "Sum = 0\n",
    "\n",
    "for test in range(100):\n",
    "    meanRT = simDecayVarDf.rt.mean()\n",
    "    stdDevRT = simDecayVarDf.rt.std()\n",
    "\n",
    "    print(\"RT (average) = {:.0f} ms\".format(meanRT/dt))\n",
    "    print('stdDev = {:.0f} ms'.format(stdDevRT/dt))\n",
    "    simDecayVarDf, traces = sim_decay_ddm_trials(parameters_decay, ntrials, deadline)\n",
    "    cs,p = getChiSquare(dataVar, simDecayVarDf)\n",
    "    print()\n",
    "    Sum += p\n",
    "print(\"Mean p-value:\", (Sum/100))\n",
    "\n",
    "varx = vis.plot_this_sims(df, parameters,traces = traces, plot_v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple Single-Bound DDM - Non Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = .5 # boundary height\n",
    "v = .0235 # strong drift-rate\n",
    "tr = .15 # nondecision time (in seconds)\n",
    "z = 0 # starting point ([0,1], fraction of a)\n",
    "\n",
    "dt = .001 # time step\n",
    "si = .3 # sigma (noise scalar)\n",
    "dx = si * np.sqrt(dt) # evidence step\n",
    "deadline = 10 # max decision time\n",
    "ntime = np.int(np.floor(deadline / dt)) # time limit for decision\n",
    "ntrials = 233 # number of trials to simulate\n",
    "\n",
    "\n",
    "\n",
    "parameters = np.array([a, tr, v, z, si, dx, dt])\n",
    "\n",
    "simNVarDf, traces = sim_ddm_trials(parameters, ntrials, deadline)\n",
    "cs, p = getChiSquare(dataNVar, simNVarDf)\n",
    "\n",
    "Sum = 0\n",
    "\n",
    "for test in range(100):\n",
    "    meanRT = simNVarDf.rt.mean()\n",
    "    stdDevRT = simNVarDf.rt.std()\n",
    "\n",
    "    print(\"RT (average) = {:.0f} ms\".format(meanRT/dt))\n",
    "    print('stdDev = {:.0f} ms'.format(stdDevRT/dt))\n",
    "    simNVarDf, traces = sim_ddm_trials(parameters, ntrials, deadline)\n",
    "    cs,p = getChiSquare(dataVar, simNVarDf)\n",
    "    print()\n",
    "    Sum += p\n",
    "print(\"Mean p-value:\", (Sum/100))\n",
    "\n",
    "# varx = vis.plot_this_sims(df, parameters,traces = traces, plot_v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decay Single-Bound DDM - Non Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = .5 # boundary height\n",
    "v = .0235 # strong drift-rate\n",
    "tr = .15 # nondecision time (in seconds)\n",
    "z = 0 # starting point ([0,1], fraction of a)\n",
    "\n",
    "dt = .001 # time step\n",
    "si = .3 # sigma (noise scalar)\n",
    "dx = si * np.sqrt(dt) # evidence step\n",
    "deadline = 10 # max decision time\n",
    "ntime = np.int(np.floor(deadline / dt)) # time limit for decision\n",
    "ntrials = 233 # number of trials to simulate\n",
    "decay = 0.000005\n",
    "decay_acceleration = 0.00001\n",
    "\n",
    "\n",
    "parameters_decay = np.array([a, tr, v, z, si, dx, dt, decay, decay_acceleration])\n",
    "\n",
    "simDecayNVarDf, traces = sim_decay_ddm_trials(parameters_decay, ntrials, deadline)\n",
    "cs, p = getChiSquare(dataVar, simDecayNVarDf)\n",
    "\n",
    "Sum = 0\n",
    "\n",
    "for test in range(100):\n",
    "    meanRT = simDecayNVarDf.rt.mean()\n",
    "    stdDevRT = simDecayNVarDf.rt.std()\n",
    "\n",
    "    print(\"RT (average) = {:.0f} ms\".format(meanRT/dt))\n",
    "    print('stdDev = {:.0f} ms'.format(stdDevRT/dt))\n",
    "    simDecayNVarDf, traces = sim_decay_ddm_trials(parameters_decay, ntrials, deadline)\n",
    "    cs,p = getChiSquare(dataNVar, simDecayNVarDf)\n",
    "    print()\n",
    "    Sum += p\n",
    "print(\"Mean p-value:\", (Sum/100))\n",
    "\n",
    "# varx = vis.plot_this_sims(df, parameters,traces = traces, plot_v=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO\n",
    "\n",
    "+ Estimate parameters for models (show before and after to show improvement in chi squares)\n",
    "    + write function that automatically calculates the chi squared against a defined experimental data set (all, var, Nvar)\n",
    "    + MVP (DDM for Var vs Nvar)\n",
    "+ Beyond 1: QQ plots (prolly not lol)\n",
    "+ Results section\n",
    "+ Conclusion \n",
    "+ Write Background and Problem sections"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Future Work "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plots for initial vs End data \n",
    "\n",
    "notice how the end distribution is shifted more towards the right"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataisInit = data[data.timePeriod == 'Initial']\n",
    "dataisEnd = data[data.timePeriod == 'End']\n",
    "dataInit,dataEnd = changeD(dataisInit),changeD(dataisEnd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "initialColor = '#FCC800'\n",
    "initDataAx = sns.distplot(dataInit.rt.values, color='orange', label='Initial')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "endColor = '#00479D'\n",
    "endDataAx = sns.distplot(dataEnd.rt.values, color='b', label='End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "initDataAx = sns.distplot(dataInit.rt.values, color='orange', label='Initial')\n",
    "endDataAx = sns.distplot(dataEnd.rt.values, color='b', label='End')\n",
    "legend, title = plt.legend(), plt.title('Initial Period vs End')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
